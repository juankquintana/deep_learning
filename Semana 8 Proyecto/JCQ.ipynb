{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "797f3457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, add\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c6bbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134260544 (512.16 MB)\n",
      "Trainable params: 134260544 (512.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=VGG16()\n",
    "model=Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d81d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'Images'\n",
    "\n",
    "features = {}\n",
    "\n",
    "for img_name in os.listdir(image_dir):\n",
    "    filename = os.path.join(image_dir, img_name)\n",
    "    \n",
    "    # Cargar y procesar imagen\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    # Extraer características\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    img_id = img_name.split('.')[0]  # ejemplo: '123456' de '123456.jpg'\n",
    "    features[img_id] = feature.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c96740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15d1fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_path = 'captions.txt'\n",
    "descriptions = {}\n",
    "\n",
    "with open(caption_path, 'r') as file:\n",
    "    next(file)  # Saltar encabezado\n",
    "    for line in file:\n",
    "        tokens = line.strip().split(',')\n",
    "        img_id, caption = tokens[0], tokens[1]\n",
    "        img_id = img_id.split('.')[0]\n",
    "        if img_id not in descriptions:\n",
    "            descriptions[img_id] = []\n",
    "        # Puedes agregar tokens especiales para entrenar el modelo\n",
    "        descriptions[img_id].append('startseq ' + caption + ' endseq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea78212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_captions(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, caps in descriptions.items():\n",
    "        new_caps = []\n",
    "        for caption in caps:\n",
    "            caption = caption.lower()\n",
    "            caption = caption.translate(table)\n",
    "            caption = caption.split()\n",
    "            caption = [word for word in caption if len(word) > 1 and word.isalpha()]\n",
    "            new_caps.append(' '.join(caption))\n",
    "        descriptions[key] = new_caps\n",
    "\n",
    "clean_captions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e19b1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Crear lista de todas las descripciones\n",
    "all_captions = []\n",
    "for key in descriptions:\n",
    "    all_captions.extend(descriptions[key])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "# Guardar tokenizer\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a3898",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = {}\n",
    "\n",
    "with open('captions.txt', 'r') as file:\n",
    "    next(file)  # Saltamos la cabecera si existe\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        img_id, caption = line.split(',', 1)\n",
    "        img_id = img_id.split('.')[0]  # Quita extensión .jpg\n",
    "\n",
    "        # Agrega tokens especiales\n",
    "        caption = 'startseq ' + caption.lower() + ' endseq'\n",
    "\n",
    "        if img_id not in descriptions:\n",
    "            descriptions[img_id] = []\n",
    "        descriptions[img_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e82968",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_captions(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, captions in descriptions.items():\n",
    "        new_captions = []\n",
    "        for caption in captions:\n",
    "            caption = caption.lower()\n",
    "            caption = caption.translate(table)\n",
    "            caption = caption.split()\n",
    "            caption = [word for word in caption if len(word) > 1 and word.isalpha()]\n",
    "            new_captions.append(' '.join(caption))\n",
    "        descriptions[key] = new_captions\n",
    "\n",
    "clean_captions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aac6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"descriptions.pkl\", \"wb\") as f:\n",
    "    pickle.dump(descriptions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb9b5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Cargar features\n",
    "with open(\"features.pkl\", \"rb\") as f:\n",
    "    features = pickle.load(f)\n",
    "\n",
    "# Cargar descripciones\n",
    "with open(\"descriptions.pkl\", \"rb\") as f:\n",
    "    descriptions = pickle.load(f)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2a64ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_length(descriptions):\n",
    "    return max(len(caption.split()) for desc in descriptions.values() for caption in desc)\n",
    "\n",
    "max_len = max_length(descriptions)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd21a4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8574\n",
      "Max caption length: 34\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "def max_length(descriptions):\n",
    "    return max(len(d.split()) for desc in descriptions.values() for d in desc)\n",
    "\n",
    "max_len = max_length(descriptions)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Max caption length:\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8871059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences, to_categorical\n",
    "import numpy as np\n",
    "\n",
    "def data_generator(descriptions, features, tokenizer, max_len, batch_size):\n",
    "    while True:\n",
    "        X1, X2, y = [], [], []\n",
    "        n = 0\n",
    "        for key, desc_list in descriptions.items():\n",
    "            for desc in desc_list:\n",
    "                seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_len)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                    X1.append(features[key])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "\n",
    "                    n += 1\n",
    "                    if n == batch_size:\n",
    "                        yield ([np.array(X1), np.array(X2)], np.array(y))\n",
    "                        X1, X2, y = [], [], []\n",
    "                        n = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e40d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 34)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 4096)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 34, 256)              2194944   ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 4096)                 0         ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 34, 256)              0         ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  1048832   ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 256)                  525312    ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 256)                  0         ['dense[0][0]',               \n",
      "                                                                     'lstm[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 256)                  65792     ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 8574)                 2203518   ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6038398 (23.03 MB)\n",
      "Trainable params: 6038398 (23.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "\n",
    "# Imagen\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# Texto\n",
    "inputs2 = Input(shape=(max_len,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# Fusionar\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ef47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "463/632 [====================>.........] - ETA: 20s - loss: 5.8240"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "steps = sum(len(c) for c in descriptions.values()) // batch_size\n",
    "\n",
    "generator = data_generator(descriptions, features, tokenizer, max_len, batch_size)\n",
    "\n",
    "model.fit(generator, epochs=20, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c15253",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"caption_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def generate_caption(model, tokenizer, photo, max_len):\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_len):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_len)\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word.get(yhat)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text.replace('startseq ', '').replace(' endseq', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de806ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '1000268201_693b08cb0e'  # solo el nombre sin .jpg\n",
    "photo = features[img_id]\n",
    "photo = photo.reshape((1, 4096))\n",
    "\n",
    "caption = generate_caption(model, tokenizer, photo, max_len=34)  # usa el max_len que calculaste antes\n",
    "print(\"Predicción:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab2d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ruta a la imagen original\n",
    "image_path = f\"Images/{img_id}.jpg\"\n",
    "\n",
    "# Mostrar imagen + caption\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(caption)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
